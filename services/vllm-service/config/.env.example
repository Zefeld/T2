# vLLM Service Environment Variables
# Copy this file to .env and modify as needed

# Model Configuration
MODEL_NAME=microsoft/DialoGPT-medium
MODEL_PATH=/app/models
MAX_MODEL_LEN=2048
GPU_MEMORY_UTILIZATION=0.9
TRUST_REMOTE_CODE=false

# Server Configuration
VLLM_HOST=0.0.0.0
VLLM_PORT=8000
ENABLE_CORS=true

# Parallel Processing
TENSOR_PARALLEL_SIZE=1
PIPELINE_PARALLEL_SIZE=1

# Performance Settings
BLOCK_SIZE=16
SWAP_SPACE=4
MAX_NUM_BATCHED_TOKENS=2048
MAX_NUM_SEQS=256

# GPU Configuration
CUDA_VISIBLE_DEVICES=all
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Logging
LOG_LEVEL=INFO
PYTHONUNBUFFERED=1
DISABLE_LOG_STATS=false

# Ray Configuration (if using distributed inference)
VLLM_WORKER_USE_RAY=false
RAY_ADDRESS=

# Quantization (optional)
QUANTIZATION_METHOD=none
LOAD_FORMAT=auto

# Memory Management
KV_CACHE_DTYPE=auto
QUANTIZED_KV_CACHE=false
PREEMPTION_MODE=swap

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=8001

# Security
DISABLE_SAFETY_CHECKER=false
MAX_INPUT_LENGTH=2048

# HuggingFace Configuration
HF_HOME=/root/.cache/huggingface
TRANSFORMERS_CACHE=/root/.cache/huggingface/transformers
HF_DATASETS_CACHE=/root/.cache/huggingface/datasets

# Optional: API Keys (if needed for private models)
# HUGGINGFACE_TOKEN=your_token_here
# OPENAI_API_KEY=your_key_here