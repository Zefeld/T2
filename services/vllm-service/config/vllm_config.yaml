# vLLM Configuration for SciBox Deployment
# This file contains the main configuration settings for the vLLM service

# Model Configuration
model:
  # Default model (can be overridden via environment variables)
  name: "microsoft/DialoGPT-medium"
  # Path where models are stored
  path: "/app/models"
  # Maximum sequence length
  max_model_len: 2048
  # GPU memory utilization (0.0 to 1.0)
  gpu_memory_utilization: 0.9
  # Trust remote code (be careful with this setting)
  trust_remote_code: false
  # Revision/branch to use
  revision: "main"

# Parallel Processing
parallel:
  # Number of tensor parallel processes
  tensor_parallel_size: 1
  # Number of pipeline parallel processes  
  pipeline_parallel_size: 1
  # Distributed backend
  distributed_backend: "nccl"

# Performance Settings
performance:
  # Block size for memory management
  block_size: 16
  # Swap space in GB
  swap_space: 4
  # Maximum number of batched tokens
  max_num_batched_tokens: 2048
  # Maximum number of sequences
  max_num_seqs: 256
  # Enable chunked prefill
  enable_chunked_prefill: true
  # Maximum number of prefill tokens
  max_num_prefill_tokens: 4096

# Server Configuration
server:
  # Host to bind to
  host: "0.0.0.0"
  # Port to listen on
  port: 8000
  # Enable CORS
  enable_cors: true
  # Allowed origins for CORS
  allowed_origins: ["*"]
  # Allowed methods for CORS
  allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
  # Allowed headers for CORS
  allowed_headers: ["*"]

# Logging Configuration
logging:
  # Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  level: "INFO"
  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  # Disable log stats
  disable_log_stats: false
  # Log file path
  log_file: "/app/logs/vllm.log"

# Engine Configuration
engine:
  # Use Ray for distributed inference
  worker_use_ray: false
  # Ray address (if using Ray)
  ray_address: null
  # Engine use async output processor
  engine_use_async_output_processor: true
  # Disable log requests
  disable_log_requests: false

# Tokenizer Configuration
tokenizer:
  # Tokenizer mode
  tokenizer_mode: "auto"
  # Skip tokenizer init
  skip_tokenizer_init: false
  # Tokenizer revision
  tokenizer_revision: null

# Quantization Settings
quantization:
  # Quantization method (none, awq, gptq, squeezellm, fp8)
  method: "none"
  # Load format (auto, pt, safetensors, npcache, dummy)
  load_format: "auto"

# Memory Management
memory:
  # KV cache data type
  kv_cache_dtype: "auto"
  # Quantized KV cache
  quantized_kv_cache: false
  # Preemption mode
  preemption_mode: "swap"

# Safety and Security
safety:
  # Disable safety checker
  disable_safety_checker: false
  # Maximum input length
  max_input_length: 2048
  # Enable prefix caching
  enable_prefix_caching: false

# Monitoring
monitoring:
  # Enable metrics
  enable_metrics: true
  # Metrics port
  metrics_port: 8001
  # Health check endpoint
  health_check_endpoint: "/health"